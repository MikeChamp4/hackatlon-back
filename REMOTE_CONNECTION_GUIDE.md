# üîó Configuraci√≥n para Conectar a PC Remoto

## üìç **¬øD√≥nde se llama al modelo?**

El modelo se llama desde **`app/services/chat_service.py`** en la l√≠nea **37**:

```python
# L√≠nea 37 - Aqu√≠ es donde se hace la llamada
response = client.chat(
    model=service.model_name,
    messages=[
        {
            'role': 'user',
            'content': query
        }
    ]
)
```

## üåê **C√≥mo conectarte al PC de tu compa√±ero**

### **Paso 1: Configurar la URL de Ollama**

Crea un archivo `.env` en la ra√≠z del proyecto:

```bash
cp .env.example .env
```

### **Paso 2: Editar el archivo .env**

```bash
# Cambia esta l√≠nea en el archivo .env:
OLLAMA_HOST=http://IP_DE_TU_COMPA√ëERO:11434

# Ejemplos:
# OLLAMA_HOST=http://192.168.1.100:11434
# OLLAMA_HOST=http://10.0.0.50:11434
# OLLAMA_HOST=http://172.16.0.25:11434
```

### **Paso 3: Obtener la IP del PC de tu compa√±ero**

Tu compa√±ero debe ejecutar en su PC:

```bash
# En Linux/Mac
ip addr show | grep inet

# En Windows
ipconfig

# O m√°s simple
hostname -I
```

### **Paso 4: Configurar Ollama en el PC de tu compa√±ero**

Tu compa√±ero debe configurar Ollama para aceptar conexiones externas:

#### **Opci√≥n A: Variable de entorno (Recomendado)**
```bash
export OLLAMA_HOST=0.0.0.0:11434
ollama serve
```

#### **Opci√≥n B: Servicio systemd (Linux)**
```bash
# Editar el archivo de servicio
sudo systemctl edit ollama

# Agregar:
[Service]
Environment="OLLAMA_HOST=0.0.0.0:11434"

# Reiniciar
sudo systemctl restart ollama
```

### **Paso 5: Verificar la conexi√≥n**

```bash
# Desde tu PC, prueba la conexi√≥n
curl http://IP_DE_TU_COMPA√ëERO:11434/api/tags

# Debe devolver la lista de modelos disponibles
```

### **Paso 6: Probar el endpoint**

```bash
# Ejecutar el script de pruebas
OLLAMA_HOST=http://IP_DE_TU_COMPA√ëERO:11434 python test_chat.py

# O probar directamente
curl -X POST http://localhost:8000/chat \
  -H "Content-Type: application/json" \
  -d '{"query": "Hola desde remoto!"}'
```

## üî• **Ejemplo completo**

Si tu compa√±ero tiene IP `192.168.1.100`, tu archivo `.env` debe tener:

```bash
OLLAMA_HOST=http://192.168.1.100:11434
```

Y tu compa√±ero debe ejecutar:

```bash
export OLLAMA_HOST=0.0.0.0:11434
ollama serve
```

## üö® **Troubleshooting**

### **Error de conexi√≥n**
```bash
# Verificar que el puerto est√© abierto
telnet IP_DE_TU_COMPA√ëERO 11434

# Verificar firewall (tu compa√±ero)
sudo ufw allow 11434
```

### **Modelo no encontrado**
```bash
# Tu compa√±ero debe tener el modelo
ollama pull gemma:7b
ollama list
```

### **Verificar configuraci√≥n actual**
```bash
# Ver la configuraci√≥n actual
curl http://localhost:8000/chat/model-info
```

## üìù **Resumen del flujo**

1. **Tu compa√±ero**: Configura Ollama para aceptar conexiones externas
2. **Tu compa√±ero**: Tiene el modelo Gemma:7B descargado
3. **T√∫**: Configuras `OLLAMA_HOST` en tu `.env`
4. **T√∫**: Ejecutas tu aplicaci√≥n Flask
5. **Frontend**: Env√≠a requests a tu Flask API
6. **Tu Flask API**: Se conecta al Ollama de tu compa√±ero
7. **Respuesta**: Viene del modelo en el PC de tu compa√±ero

```
Frontend ‚Üí Tu Flask API ‚Üí PC Compa√±ero (Ollama + Gemma:7B) ‚Üí Respuesta
```
